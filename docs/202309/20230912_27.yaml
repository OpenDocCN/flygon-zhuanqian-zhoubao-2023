- qas:
  - 问题1：什么是 Llama2？
  - 回答1：Llama2 是由 Meta 开源的一个大语言模型，是最近几个月一系列开源模型的基础模型。
  - 问题2：Llama2 相对于 GPT 的优势在哪里？
  - 回答2：从测评表现来看，Llama2 距离 GPT 还有一些差距，但在进步很快。
  - 问题3：Llama2 有哪些不同参数量级的版本？
  - 回答3：Llama2 的版本有 7B, 13B 和 70B，其中最大的参数规模是 700 亿参数。
  - 问题4：如何选择合适的服务器配置来部署 Llama2？
  - 回答4：建议使用 A40 显存 40G 以上的配置来部署 70B 版本，可以确保运行流畅。
  - 问题5：如何开始部署 Text generation web UI？
  - 回答5：可以通过选择适当的容器实例并配置基础镜像 PyTorcha，然后进入 Jupyter 进行部署操作。
  text: '# 以 Llama2 为例，教你部署自己的私有大模型> 原文：[`www.yuque.com/for_lazy/thfiu8/vnq4w0y09bbb6r73`](https://www.yuque.com/for_lazy/thfiu8/vnq4w0y09bbb6r73)##
    (25 赞)以 Llama2 为例，教你部署自己的私有大模型作者： 小辉 AI 教练日期：2023-09-04## 一、背景介绍：一夜之间，AI 大模型圈变了天。Meta
    联手微软宣布，Llama 2 正式开源商用。LLaMA 是由 Meta 开源的一个大语言模型，是最近几个月一系列开源模型的基础模型。包括著名的 vicuna
    系列、LongChat 系列等都是基于该模型微调得到。可以说，LLaMA 的开源促进了大模型在开源界繁荣发展。而刚刚，微软官方宣布 Azure 上架 LLaMA2
    模型！这意味着 LLaMA2 正式发布！无论对于我们个人还是企业来说，都是一个革命性的进步，虽然国内还沉浸在百模大战中。我们现在可以像玩 Stable Diffusion
    一样部署自己的私有模型，和进行微调训练 Lora 模型了。之前我们想去做垂直行业的模型训练，需要调用 GPT 的接口，而且还需要科学上网，现在你可以自己部署一个自己的私有模型，并进行自己的定制，做上层的应用，而不是基于
    GPT 的简单套壳。那么对比 GPT 来说，Llama 达到了什么水平，从测评表现来看，Llama2 距离 GPT 还是有一些差距，但是 Llama 的进步还是很快的，从第一代发布到第二代仅用了几个月![](img/72a9939c5a1a3ccecf141a60de7a7835.png)![](img/3037591bb991934878232de14eb5d88a.png)![](img/cdc25a7a620878f707abef16b6b198d2.png)今天这篇教程，就是教大家如何部署
    Llama2, 有 3 个参数量级的版本可以选，分别为：7B, 13B, 和 70B，我们就以最大的参数规模 700 亿参数来举例，7B 模型文件有 7 个
    G 左右，在 A5000 显卡测下来还比较流畅，70B 的模型文件就有 35G 大小，所以显存最低要 40G，在 A40 48G 配置的机器上测下拉比较流畅。![](img/603db663cd403b6bab645d74067d7c62.png)##
    二、实操部署1.  服务器配置我们就以 AutoDL 的云服务来部署，其他云厂商的都差不多。首先我们去算力市场选一台配置够用的容器实例，看你部署的版本，70B
    建议 A40 显存 40G 以上，2.8 一小时。![](img/fa2732e2ba00d571be090e5ff453add6.png)选好可用的配置后，点击租用，进行下一步配置![](img/b0ac940029ef4666b2080fa5770f8d08.png)这里我们选择基础镜像
    PyTorcha 就可以了，注意不能选最高版本，系统版本不支持，然后点击“立即创建”等个 1 分钟左右，就可以看到你的容器已经启动好了：![](img/8418574e2f4cb86dfad671a20e9942c3.png)接下来我们选择
    Jupyter，进入 ternimal 终端，可以开始部署操作了![](img/3005834b7f15637a60c70042d0d6e76a.png)1.  Text
    generation web UI 部署'
- qas:
  - 问题1：仓库地址是什么？
  - 回答1：[`github.com/AYLIEN/text-generation-webui`](https://github.com/AYLIEN/text-generation-webui)
  - 问题2：如何安装 Conda？
  - 回答2：参考官方文档 [`docs.conda.io/en/latest/miniconda.html`](https://docs.conda.io/en/latest/miniconda.html)，在
    Linux 或 WSL 上，可以使用两个命令自动安装。
  - 问题3：如何创建 Conda 环境？
  - 回答3：需要执行创建环境的命令。
  - 问题4：如何安装依赖？
  - 回答4：需要执行安装依赖的步骤。
  - 问题5：哪里可以下载已转换好的模型文件？
  - 回答5：可以直接在 Hugging Face 上下载，使用以下链接：
  text: 然后把 Text generation web UI 仓库([`github.com/AYLIEN/text-generation-webui`](https://github.com/AYLIEN/text-generation-webui))克隆到服务器，下一步就是根据官方文档安装环境和依赖：1.  安装
    **Conda**2.  [`docs.conda.io/en/latest/miniconda.html`](https://docs.conda.io/en/latest/miniconda.html)3.  On
    Linux or WSL, it can be automatically installed with these two commands ([source](https://educe-ubc.github.io/conda.html)):1.  **创建一个
    conda 环境**1.  安装依赖1.  模型下载界面我们已经有了，然后就是把我们的模型安装就可以了，跟 SD 一样，你只要把模型下载到 models 目录下,
    UI 就可以加载了。![](img/2766108c7b5343756f6c81c6b838ed25.png)Meta 官方下载模型的话需要申请，在他们官网上填申请表就可以了，等几分钟，他们会把下载的
    key 发到你邮箱里![](img/2a94514e1d14da318db8673a3b47039e.png)当然，原生的模型文件不能在 text-generation-ui
    下运行，默认使用的是 huggingface 格式的模型，因此我们需要一次格式转换操作，或者使用已经转换好的模型文件，这里我们直接使用 TheBloke 大佬转换好的模型文件，在
    huggingface 上可以直接下载，下面是各个版本的链接：1.  [`huggingface.co/TheBloke/Llama-2-70B-chat-GPTQ`](https://huggingface.co/TheBloke/Llama-2-70B-chat-GPTQ)2.  [`huggingface.co/TheBloke/Llama-2-70B-GPTQ`](https://huggingface.co/TheBloke/Llama-2-70B-GPTQ)3.  [`huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ`](https://huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ)4.  [`huggingface.co/TheBloke/Llama-2-13B-GPTQ`](https://huggingface.co/TheBloke/Llama-2-13B-GPTQ)5.  [`huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ`](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ)6.  [`huggingface.co/TheBloke/Llama-2-7B-GPTQ`](https://huggingface.co/TheBloke/Llama-2-7B-GPTQ)##
    Hugging Face Hub 模型下载方案（优雅，强烈推荐）1.  启动 UI：这行启动命令的意思是把模型映射到“TheBloke_Llama-2-70B-GPTQ”，
- qas:
  - 问题1：为什么要监听 6006 端口？
  - 回答1：因为这个端口可以启动 AutoDL 的自定义服务。
  - 问题2：如何进入 UI？
  - 回答2：通过浏览器访问指定地址。
  - 问题3：如何加载模型？
  - 回答3：通过顶部菜单进入 Model 页面加载已下载的模型。
  - 问题4：如何确认模型加载成功？
  - 回答4：界面上显示“Successfully”表示模型加载成功。
  - 问题5：如何改变对话交互模式？
  - 回答5：可以改为熟悉的聊天模式。
  text: ' 监听 6006 端口，因为这个端口可以启动 AutoDL 的自定义服务，让我们可以通过浏览器访问![](img/1c90dc331562561600f16e2623fa010e.png)进入到
    UI 后我们能看到主界面, 我们先通过顶部菜单进入到 Model 页面加载模型，这里选择已经下载的 70B 模型![](img/fffa61c85f982c23aaf615224468b210.png)当界面上显示“Successfully”就表示模型加载成功了![](img/36724662b361fcdb6b72b801eab3d70a.png)然后我们就可以愉快的和
    Llama2 对话了![](img/83f46f165daccc6c40e6a8ea23df9b13.png)默认的对话交互模式体验不太好，我们可以改为熟悉的聊天模式![](img/8a348dfa016b30e5fb096f94d9f6b988.png)![](img/ba3a1e6dcc6f0c7417cd99130b26829f.png)Text
    generation web UI 比较好用的一个功能就是可以定制自己的角色，比如设定角色的性格等![](img/69750b494fe3fb6821a76d7f97914536.png)![](img/1e0a8350ffcf3cde189b0b2be4e19c39.png)多模型：Text
    generation web UI 是支持多个开源模型的，你只需要把各个模型下载到 models 文件夹下即可， 然后在界面上切换模型也很方便。![](img/24f3f97b98129fc0c0bd856b83722aca.png)飞书和视频教程链接：https://j0oqwdylc47.feishu.cn/docx/G2uvdNJIloP8D8xIXBccKZDunYg*
    * *评论区：暂无评论![](img/1c37d505930596d12a88ab23e11aa07a.png)* * *'
